{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-video-inference.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEz1lHr7HeGI"
      },
      "source": [
        "# Colab-video-inference\n",
        "\n",
        "My repo: [styler00dollar/Colab-video-inference](https://github.com/styler00dollar/Colab-video-inference)\n",
        "\n",
        "A basic colab to take video models to create videos. Load a jit or pth. Currently only supporting batch_size 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvW493AXINSu"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XOL2YraIN_Q"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeFByrsRI6Ew"
      },
      "source": [
        "# get data\n",
        "%cd /content\n",
        "!wget "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "u8yz0lmA5gek"
      },
      "source": [
        "#@markdown adjust code if needed\n",
        "from types import FrameType\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import torch\n",
        "import torchvision.transforms as TF\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import warnings\n",
        "import numpy\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "input_video = \"/content/m.webm\" #@param\n",
        "\n",
        "%cd /content/\n",
        "!sudo rm -rf /content/png/\n",
        "!mkdir /content/png/\n",
        "%cd /content/\n",
        "%shell ffmpeg -i {input_video} png/%05d.png\n",
        "\n",
        "frames_dir = \"/content/png/\"\n",
        "files = sorted(glob.glob(frames_dir + '/**/*.png', recursive=True))\n",
        "del files[-1]\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/CAIN/rvpv2/Checkpoint_4_55000_G.pt\" #@param\n",
        "# model, load it manually\n",
        "#model = \n",
        "#model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# load a jit\n",
        "model = torch.jit.load(model_path)\n",
        "model.cuda().eval()\n",
        "\n",
        "input_frame = 1\n",
        "for f in tqdm(files):\n",
        "  with torch.no_grad():\n",
        "    filename_frame_1 = f\n",
        "    filename_frame_2 = os.path.join(frames_dir, f'{input_frame+1:0>5d}.png')\n",
        "    output_frame_file_path = os.path.join(frames_dir, f\"{input_frame:0>5d}_0.5.png\")\n",
        "\n",
        "    img1 = cv2.imread(filename_frame_1)\n",
        "    img2 = cv2.imread(filename_frame_2)\n",
        "\n",
        "    # resize input\n",
        "    img1 = cv2.resize(img1, (1280, 720)) #, interpolation=cv2.INTER_NEAREST)\n",
        "    img2 = cv2.resize(img2, (1280, 720)) #, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    img1_new = cv2.resize(img1, (1280, 720)) #, interpolation=cv2.INTER_NEAREST)\n",
        "    img2_new = cv2.resize(img2, (1280, 720)) #, interpolation=cv2.INTER_NEAREST)\n",
        "    cv2.imwrite(filename_frame_1, img1_new)\n",
        "    cv2.imwrite(filename_frame_2, img2_new)\n",
        "\n",
        "    #img1 = cv2.cvtColor(img1, cv2.COLOR_RGB2YUV)\n",
        "    #img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2YUV)\n",
        "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img1 = torch.from_numpy(img1).unsqueeze(0).permute(0,3,1,2)/255\n",
        "    img2 = torch.from_numpy(img2).unsqueeze(0).permute(0,3,1,2)/255\n",
        "\n",
        "    out = model(img1.cuda().contiguous(), img2.cuda().contiguous())\n",
        "\n",
        "    # to numpy and save\n",
        "    out = out.data.mul(255).mul(255 / 255).clamp(0, 255).round()\n",
        "    out = out.squeeze(0).permute(1, 2, 0).cpu().numpy() #*255\n",
        "    out = out.astype(np.uint8)\n",
        "    #out = cv2.cvtColor(out, cv2.COLOR_YUV2RGB)\n",
        "    out = cv2.cvtColor(out, cv2.COLOR_RGB2BGR)\n",
        "    out = cv2.resize(out, (1280,720)) #, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    cv2.imwrite(output_frame_file_path, out)\n",
        "\n",
        "    input_frame += 1\n",
        "\n",
        "%cd /content/png/\n",
        "%shell ffmpeg -y -r 48 -f image2 -pattern_type glob -i '/content/png/*.png' -crf 18 /content/out.mp4\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kerxb30_Itvu"
      },
      "source": [
        "# create jit, assuming you defined the model prior\n",
        "traced_model = torch.jit.trace(model, (torch.randn(1,3,720,1280).cuda(),torch.randn(1,3,720,1280).cuda()))\n",
        "torch.jit.save(traced_model, \"traced2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}